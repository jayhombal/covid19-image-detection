{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS6050 Deep Learning Project\n",
    "__Project: Covid19 Image Classification - Phase 1__\n",
    "__Team: Paul Hicks(pdh2d), Sudharshan Luthra(sl3zs), Jay Hombal(mh4ey), Matt Dakolios(mrd7f)__\n",
    "\n",
    "__Abstract:__ Our Aim is to detect Covid19 from chest X-rays. The covid19 image dataset we are using is small with about 3000 classes belonging to three classes  'Normal', 'Covid19' and 'Pneumonia' respectively. This dataset small and is insufficient to generalize. So for the purpoe of our project, in Phase-I we will first use NIH X-ray image data to retrain and finetune pretrained model architecture such as ResNet50V2, MobileNetV2 and VGG16.\n",
    "\n",
    "In Phase2, we intend to reload the best saved model from Phase 1 to train, validate, finetune the model and finally evaluate the classifications on the target covid19 dataset.\n",
    "\n",
    "Reference:\n",
    "1. https://www.kaggle.com/nih-chest-xrays/data\n",
    "2. https://www.kaggle.com/mushaxyz/covid19-customized-xray-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Code Orginzation\n",
    "\n",
    "Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates), e.g. Python package projects, LaTeX documents, etc.\n",
    "  \n",
    "__Installed and created the project template using Cookiecutter:__  \n",
    "Follow instructions from https://ericbassett.tech/cookiecutter-data-science-crash-course/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating and pre-processing NIH X-ray metadata dataset  \n",
    "\n",
    "Following instructions use make tool, run commands from from your terminal from your project folder\n",
    "\n",
    "__Setup__  \n",
    "Setup python environment   \n",
    "\n",
    "    1. Validate Python is installed and create required directories  \n",
    "        Run: make test_environment  \n",
    "\n",
    "__Data Extraction:(execute only once)__  \n",
    "\n",
    "    2. Download and unzip the NIH X-ray images in data/raw    \n",
    "        Run: make get_nih_images   \n",
    "\n",
    "__Data Validation:(execute only once)__  \n",
    "\n",
    "    3. Validate Dataset (rename columns and delete patient record with age greater than 100)   \n",
    "        Run: make validate_nih_images   \n",
    "\n",
    "__Data Prepartion:(execute only once)__  \n",
    "\n",
    "    4. Prepare Dataset (add path attribute, split dataset into train and validation dataset)\n",
    "        Run: make prepare_nih_images\n",
    "\n",
    "This proudces the three output files in processed folder:\n",
    "    1. prepared_data_entry_2017.csv (full dataset)\n",
    "    2. prepared_train_data_entry_2017.csv (train_dataset)\n",
    "    3. prepared_valid_data_entry_2017.csv (validation_dataset)\n",
    "\n",
    "Next, we use prepared_train_data_entry_2017.csv and prepared_valid_data_entry_2017.csv files to retrain CNN model architectures pre-trained using IMAGENET database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "# prevent VRAM occupied\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# TensorFlow â‰¥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Change working directory  - as the images are located in data/raw in the project folder __(Execute this cell Once)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if '/notebooks' in os.getcwd():\n",
    "    os.chdir(\"../\")\n",
    "    print(\"set the project directory as working directory\")\n",
    "else:\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for trianing the model\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import src.models.train_model as train_model\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED =42\n",
    "IMAGE_SIZE = (224,224)\n",
    "IMAGE_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "NUM_CLASSES = 15 # number of ClassesNUM\n",
    "NUM_EPOCHS = 10\n",
    "PRETRAINED_MODELS = ['ResNet50V2', 'MobileNetV2', 'VGG16']\n",
    "\n",
    "# Train and validate function\n",
    "def train_and_validate_model(model_name,\n",
    "                             train_generator, \n",
    "                             valid_generator, \n",
    "                             save_model_filepath: str,\n",
    "                             logs_dir: str,\n",
    "                             freeze_layers:bool = True, \n",
    "                             activation: str = 'softmax', \n",
    "                             learning_rate: float =0.01, \n",
    "                             fine_tune_learning_rate: float = 0.0001,\n",
    "                             fine_tune_at_layer:int = 186,\n",
    "                             num_epochs:int = NUM_EPOCHS,\n",
    "                             num_classes: int = NUM_CLASSES,\n",
    "                             batch_size: int = BATCH_SIZE,\n",
    "                             input_shape: int = IMAGE_SHAPE):\n",
    "    \n",
    "    print(model_name)\n",
    "    \n",
    "    my_model = train_model.get_base_model_with_new_toplayer(base_model=model_name,\n",
    "                                                          freeze_layers = freeze_layers, \n",
    "                                                          num_classes = num_classes,\n",
    "                                                          activation_func=activation,\n",
    "                                                          learning_rate = learning_rate,\n",
    "                                                          input_shape = input_shape)\n",
    "\n",
    "    my_model_history = train_model.fit_model(my_model, \n",
    "                                             train_generator, \n",
    "                                             valid_generator,\n",
    "                                             num_epochs=num_epochs,\n",
    "                                             batch_size=batch_size,\n",
    "                                             checkpoint_filepath=save_model_filepath,\n",
    "                                             logs_dir = logs_dir)\n",
    "\n",
    "    print(f'{model_name} Accuracy and Loss plots')\n",
    "    train_model.plot_accuracy_and_loss(my_model_history)\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n",
    "    #fine_tune model_name\n",
    "    model_ft = train_model.fine_tune_model(my_model,\n",
    "                                           fine_tune_learning_rate,\n",
    "                                           optimizer='Adam',\n",
    "                                           fine_tune_at_layer=fine_tune_at_layer,\n",
    "                                           activation_func=activation)\n",
    "    \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'Fine-Tuned {model_name} Training and Validation: ')\n",
    "    model_ft_history = train_model.fit_model(model_ft, train_generator, \n",
    "            valid_generator, num_epochs=num_epochs,batch_size=batch_size)\n",
    "    print(f'Fine-Tuned {model_name} Accuracy and Loss plots')\n",
    "    train_model.plot_accuracy_and_loss(model_ft_history)\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    nih_xrays_train_df = pd.read_csv('data/processed/prepared_train_data_entry_2017.csv')\n",
    "    nih_xrays_valid_df = pd.read_csv('data/processed/prepared_valid_data_entry_2017.csv')\n",
    "    return nih_xrays_train_df,nih_xrays_valid_df\n",
    "nih_xrays_train_df, nih_xrays_valid_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fourteen unique diagnosis\n",
    "# It is a function that takes a series of iterables and returns one iterable\n",
    "# The asterisk \"*\" is used in Python to define a variable number of arguments. \n",
    "# The asterisk character has to precede a variable identifier in the parameter list \n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*nih_xrays_train_df['finding_label'].map(lambda x: x.split('|')).tolist())))\n",
    "# remove the empty label\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "print('All Labels ({}): {}'.format(len(all_labels), all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_generator = train_model.get_image_data_generator(nih_xrays_train_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=all_labels,shuffle=True,seed=SEED)\n",
    "valid_generator = train_model.get_image_data_generator(nih_xrays_train_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=all_labels,shuffle=True,seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_generator)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n",
    "    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) \n",
    "                             if n_score>0.5]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Classification using all NIH data\n",
    "  \n",
    "train_model.py model includes all functions for training the model (src/models/train_model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNetV250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[0]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp1.h5'\n",
    "logs_dir = 'logs/fit/ResNet50V2exp1'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=train_generator, \n",
    "                                 valid_generator=valid_generator, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[1]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp1.h5'\n",
    "logs_dir = 'logs/fit/MobileNetV2exp1'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=train_generator, \n",
    "                                 valid_generator=valid_generator, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[2]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp1.h5'\n",
    "logs_dir = 'logs/fit/VGG16exp1'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=train_generator, \n",
    "                                 valid_generator=valid_generator, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_xrays_df = pd.read_csv('data/processed/prepared_data_entry_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_weights(df, all_labels, num_samples: int = 40000):\n",
    "    for lbl in all_labels: \n",
    "        df[lbl] = df['finding_label'].map(lambda find: 1 if lbl in find else 0)\n",
    "    df['encoding'] = [[1 if l in lbl.split('|') else 0 for l in all_labels] for lbl in nih_xrays_df['finding_label']]\n",
    "\n",
    "    class_count = {}\n",
    "    for lbl in all_labels:\n",
    "        class_count[lbl] = df[lbl].sum()\n",
    "\n",
    "    classweight = {}\n",
    "    for lbl in all_labels :\n",
    "        classweight[lbl] = 1/class_count[lbl]\n",
    "\n",
    "    classweight['NoFinding'] /= 2   #Extra penalising the none class \n",
    "    def apply_weights(row):\n",
    "        weight = 0\n",
    "        for lbl in all_labels: \n",
    "            if(row[lbl]==1):\n",
    "                weight += classweight[lbl]\n",
    "        return weight\n",
    "    new_weights = df.apply(apply_weights, axis=1)\n",
    "    sampled_data = df.sample(50000, weights = new_weights)\n",
    "\n",
    "    \n",
    "    nih_required_columns = {\n",
    "            'patient_id',\n",
    "            'image_name',\n",
    "            'path',\n",
    "            'finding_label'\n",
    "        }\n",
    "\n",
    "  \n",
    "    sampled_data = sampled_data[nih_required_columns]\n",
    "\n",
    "    \n",
    "    group_shuffle_split = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "\n",
    "    for train_idx, valid_idx in group_shuffle_split.split(sampled_data[:None],\\\n",
    "        groups=sampled_data[:None]['patient_id'].values):\n",
    "        train_df = sampled_data.iloc[train_idx]\n",
    "        valid_df = sampled_data.iloc[valid_idx]\n",
    "        \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = sample_with_weights(nih_xrays_df,all_labels,num_samples=40000)\n",
    "sampled_train_gen = train_model.get_image_data_generator(train_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=all_labels,shuffle=True,seed=SEED)\n",
    "sampled_valid_gen = train_model.get_image_data_generator(valid_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=all_labels,shuffle=True,seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[0]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp2.h5'\n",
    "logs_dir = 'logs/fit/ResNet50V2exp2'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=sampled_train_gen, \n",
    "                                 valid_generator=sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[1]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp2.h5'\n",
    "logs_dir = 'logs/fit/MobileNetV2exp2'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=sampled_train_gen, \n",
    "                                 valid_generator=sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = PRETRAINED_MODELS[2]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp2.h5'\n",
    "logs_dir = 'logs/fit/VGG16exp2'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=sampled_train_gen, \n",
    "                                 valid_generator=sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Sub Sampling Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_samples = ['Cardiomegaly','Effusion','Emphysema', 'Fibrosis', 'Infiltration', 'Pneumonia', 'Pneumothorax','Pleural_Thickening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nih_xrays_train_df  = nih_xrays_train_df [nih_xrays_train_df['finding_label'].isin(sub_samples)]\n",
    "sub_nih_xrays_valid_df = nih_xrays_valid_df[nih_xrays_valid_df['finding_label'].isin(sub_samples)]\n",
    "sub_sampled_train_gen = train_model.get_image_data_generator(sub_nih_xrays_train_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=sub_samples,shuffle=True,seed=SEED)\n",
    "sub_sampled_valid_gen = train_model.get_image_data_generator(sub_nih_xrays_valid_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=sub_samples,shuffle=True,seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[0]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp3.h5'\n",
    "logs_dir = 'logs/fit/ResNet50V2exp3'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=sub_sampled_train_gen, \n",
    "                                 valid_generator=sub_sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir,\n",
    "                                 num_classes=len(sub_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[1]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp3.h5'\n",
    "logs_dir = 'logs/fit/MobileNetV2exp3'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=sub_sampled_train_gen, \n",
    "                                 valid_generator=sub_sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir,\n",
    "                                 num_classes=len(sub_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[2]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp3.h5'\n",
    "logs_dir = 'logs/fit/VGG16exp3'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=sub_sampled_train_gen, \n",
    "                                 valid_generator=sub_sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir,\n",
    "                                 num_classes=len(sub_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Randoming reducing multiple lables to a single label for an image where multiple lables exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "reduced_nih_xrays_train_df = nih_xrays_train_df\n",
    "reduced_nih_xrays_valid_df = nih_xrays_valid_df\n",
    "reduced_nih_xrays_train_df['finding_label'] = reduced_nih_xrays_train_df['finding_label'].map( lambda x : random.choice(x.split('|')) )\n",
    "reduced_nih_xrays_valid_df['finding_label'] = reduced_nih_xrays_valid_df['finding_label'].map( lambda x : random.choice(x.split('|')) )\n",
    "reduced_sampled_train_gen = train_model.get_image_data_generator(reduced_nih_xrays_train_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=all_labels,shuffle=True,seed=SEED)\n",
    "reduced_sampled_valid_gen = train_model.get_image_data_generator(reduced_nih_xrays_valid_df,batch_size=BATCH_SIZE,image_size=IMAGE_SIZE,lables=all_labels,shuffle=True,seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[0]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp4.h5'\n",
    "logs_dir = 'logs/fit/ResNet50V2exp4'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=reduced_sampled_train_gen, \n",
    "                                 valid_generator=reduced_sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNETV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[1]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp4.h5'\n",
    "logs_dir = 'logs/fit/MobileNetV2exp4'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=reduced_sampled_train_gen, \n",
    "                                 valid_generator=reduced_sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PRETRAINED_MODELS[2]\n",
    "save_model_filepath = 'models/'+ model_name + 'exp4.h5'\n",
    "logs_dir = 'logs/fit/VGG16exp4'\n",
    "model = train_and_validate_model(model_name = model_name, \n",
    "                                 train_generator=reduced_sampled_train_gen, \n",
    "                                 valid_generator=reduced_sampled_valid_gen, \n",
    "                                 save_model_filepath=save_model_filepath,\n",
    "                                 logs_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean UP \n",
    "run this cell after completing execution of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run this cell from command prompt__  \n",
    "  \n",
    "  \n",
    "jupyter-nbconvert --to pdf COVID-19-Image-Classification-phase1.ipynb"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
