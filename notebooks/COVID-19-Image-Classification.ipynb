{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import plotly.express as px\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "# prevent VRAM occupied\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# TensorFlow â‰¥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED =42\n",
    "# minimum number of cases for each diagnosis/finding label (of 14)\n",
    "MIN_CASES  = 1000\n",
    "MIN_CASES_FLAG = True\n",
    "IMAGE_SIZE = (224,224)\n",
    "IMAGE_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "TARGET_WIDTH= 224\n",
    "TARGET_HEGIHT =224\n",
    "NUM_CLASSES = 15 # number of ClassesNUM\n",
    "NUM_EPOCHS = 20\n",
    "PRETRAINED_MODELS = ['ResNet50V2', 'MobileNetV2', 'VGG16', 'InceptionV3' ,'DenseNet121'] # pretrained modelspretrained_models = ['ResNet50V2', 'MobileNetV2', 'VGG16', 'InceptionV3'] # pretrained modelspr\n",
    "\n",
    "log_folder = 'logs' # logs folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback setup\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint_path = 'xray_class_weights.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "\n",
    "callbacks = [checkpoint, early, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_xrays_df = pd.read_csv('../data/raw/Data_Entry_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nih_xrays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "nih_column_names =  {\n",
    "    'Image Index': 'image_name',\n",
    "    'Finding Labels' : 'finding_label',\n",
    "    'Follow-up #' : 'follow_up_num',\n",
    "    'Patient ID': 'patient_id',\n",
    "    'Patient Age': 'age',\n",
    "    'Patient Gender': 'gender',\n",
    "    'View Position' : 'view_position',\n",
    "    'OriginalImage[Width': 'image_width',\n",
    "    'Height]': 'image_height',\n",
    "    'OriginalImagePixelSpacing[x' : 'x_spacing',\n",
    "    'y]': 'y_spacing'\n",
    "}\n",
    "\n",
    "nih_xrays_df =  nih_xrays_df.rename(columns=nih_column_names)\n",
    "nih_xrays_df.drop(columns=['Unnamed: 11'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_xrays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of all image paths\n",
    "image_path = '../data/raw'\n",
    "all_image_paths = {os.path.basename(x): x for x in glob(os.path.join(image_path, 'images*', '*', '*.png'))}\n",
    "\n",
    "print('count of raw images paths and rows in NIH dataset :', len(all_image_paths), ', Total Headers', nih_xrays_df.shape[0])\n",
    "\n",
    "# add image path column to dataframe\n",
    "nih_xrays_df['path'] = nih_xrays_df['image_name'].map(all_image_paths.get)\n",
    "\n",
    "# fix data errors - remove patient record with age greater than 100\n",
    "nih_xrays_df = nih_xrays_df[nih_xrays_df['age']<= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nih_xrays_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.strip(nih_xrays_df, x='age', color='finding_label',  hover_name='gender', width= 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nih_xrays_df[nih_xrays_df['age']>= 100].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.histogram(nih_xrays_df, x='age', color='finding_label',  hover_name='gender', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = nih_xrays_df['finding_label'].value_counts()[:15].reset_index()\n",
    "fig = px.bar(label_counts, x='index', y='finding_label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the 'No Finding' with '' value = why?\n",
    "nih_xrays_df['finding_label'] = nih_xrays_df['finding_label'].map(lambda x: x.replace('No Finding', 'NoFinding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nih_xrays_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fourteen unique diagnosis\n",
    "# It is a function that takes a series of iterables and returns one iterable\n",
    "# The asterisk \"*\" is used in Python to define a variable number of arguments. \n",
    "# The asterisk character has to precede a variable identifier in the parameter list \n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*nih_xrays_df['finding_label'].map(lambda x: x.split('|')).tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the empty label\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "print('All Labels ({}): {}'.format(len(all_labels), all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in all_labels:\n",
    "    nih_xrays_df[label]= nih_xrays_df['finding_label'].map(lambda finding: 1.0 if label in finding else 0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_xrays_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_label_counts ={}\n",
    "for label in all_labels:\n",
    "    finding_label_counts[label] = [nih_xrays_df[label].sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_label_counts_df = \\\n",
    "    pd.DataFrame.from_dict(finding_label_counts, \\\n",
    "                           orient='index',\\\n",
    "                           columns=['count'])\\\n",
    "                            .reset_index()\n",
    "type(finding_label_counts_df)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(finding_label_counts_df, x='index', y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Apply the min_cases logic\n",
    "\n",
    "if MIN_CASES_FLAG:\n",
    "    all_labels_with_min_cases = [label for label in all_labels \\\n",
    "                                     if nih_xrays_df[label].sum() > MIN_CASES]\n",
    "    print(f'finding labels with min cases: {len(all_labels_with_min_cases)}')  \n",
    "    print([(label, int(nih_xrays_df[label].sum())) for label in all_labels_with_min_cases])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique patients:' , nih_xrays_df['patient_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n=None\n",
    "group_shuffle_split = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "\n",
    "for train_idx, valid_idx in group_shuffle_split.split(nih_xrays_df[:n], groups=nih_xrays_df[:n]['patient_id'].values):\n",
    "    train_df = nih_xrays_df.iloc[train_idx]\n",
    "    valid_df = nih_xrays_df.iloc[valid_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of images in training dataset {train_df.shape[0]}')\n",
    "print(f'Number of images in validation dataset {valid_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=True, #Boolean. Set each sample mean to 0.\n",
    "    samplewise_std_normalization = True, #Boolean. Divide each input by its std.\n",
    "    featurewise_std_normalization=False, # divide inputs by std of the dataset\n",
    "    horizontal_flip = True, #Boolean. Randomly flip inputs horizontally.\n",
    "    vertical_flip = False,  #Boolean. Randomly flip inputs vertically.\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    height_shift_range= 0.05, #float: fraction of total height, if < 1, or pixels if >= 1.\n",
    "    width_shift_range=0.1,  #float: fraction of total height, if < 1, or pixels if >= 1.\n",
    "    rotation_range=20, #Int. Degree range for random rotations. 0 -180 degrees\n",
    "    shear_range = 0.1, #Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n",
    "    fill_mode = 'nearest', #One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'. \n",
    "    zoom_range=0.15) #Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the dataframe and the path to a directory + generates batches.\n",
    "train_generator = image_data_gen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory=None, #string, path to the directory to read images from. \n",
    "                            #If None, data in x_col column should be absolute paths.\n",
    "            x_col='path', #string, column in dataframe that contains the filenames (or absolute paths if directory is None).\n",
    "            y_col='finding_label', #string or list, column/s in dataframe that has the target data.\n",
    "    \n",
    "            class_mode=\"categorical\", #one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\". \n",
    "                              # Mode for yielding the targets: \"raw\": numpy array of values in y_col column(s),\n",
    "            classes=all_labels,\n",
    "            #color_mode='grayscale',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=SHUFFLE,\n",
    "            seed=SEED,\n",
    "            target_size= IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the dataframe and the path to a directory + generates batches.\n",
    "valid_generator = image_data_gen.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=None, #string, path to the directory to read images from. \n",
    "                            #If None, data in x_col column should be absolute paths.\n",
    "            x_col='path', #string, column in dataframe that contains the filenames (or absolute paths if directory is None).\n",
    "            y_col='finding_label', #string or list, column/s in dataframe that has the target data.\n",
    "    \n",
    "            class_mode=\"categorical\", #one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\". \n",
    "                              # Mode for yielding the targets: \"raw\": numpy array of values in y_col column(s),\n",
    "            classes=all_labels,\n",
    "            #color_mode='grayscale',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=SHUFFLE,\n",
    "            seed=SEED,\n",
    "            target_size= IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_generator.__getitem__(2)\n",
    "plt.imshow(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_generator)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n",
    "    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) \n",
    "                             if n_score>0.5]))\n",
    "    c_ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(model_name :str = 'resenet50v2', freez_layers:bool = False):\n",
    "    \"\"\" returns pretrained model\n",
    "\n",
    "    Args:\n",
    "        model_name (str): model_name values pretrained_models = ['ResNet50V2', 'MobileNetV2', 'VGG16']\n",
    "    \"\"\"\n",
    "    if model_name == 'ResNet50V2' :\n",
    "        print(f\"Downloading ResNet50V2\")\n",
    "        base_model = tf.keras.applications.ResNet50V2(input_shape=IMAGE_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    elif model_name == 'MobileNetV2' :\n",
    "        print(f\"Downloading MobileNetV2\")\n",
    "        base_model = tf.keras.applications.MobileNetV2(input_shape=IMAGE_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    elif model_name == 'VGG16' :\n",
    "        print(f\"Downloading VGG16\")\n",
    "        base_model = tf.keras.applications.VGG16(input_shape=IMAGE_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    elif model_name == 'DenseNet121' :\n",
    "        print(f\"Downloading DenseNet121\")\n",
    "        base_model = tf.keras.applications.DenseNet121(input_shape=None,\n",
    "                                                      input_tensor=None,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    if freez_layers == True:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "            #assert layer.trainable is False\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_classifier_to_base_model(base_model, num_classes: int = NUM_CLASSES):\n",
    "    \"\"\" add a classifier\n",
    "\n",
    "    Args:\n",
    "        base_model ([keras.Model]): base_model\n",
    "        num_classes ([int]) : number classes\n",
    "    \"\"\"\n",
    "    head_model = base_model.output\n",
    "    head_model = keras.layers.Flatten(name=\"flatten\")(head_model)\n",
    "    # head_model = keras.layers.Dense(256, activation=\"relu\")(head_model)\n",
    "    # head_model = keras.layers.Dropout(0.3)(head_model)\n",
    "    # head_model = keras.layers.Dense(128, activation=\"relu\")(head_model)\n",
    "    # head_model = keras.layers.Dropout(0.3)(head_model)\n",
    "    # head_model = keras.layers.Dense(64, activation=\"relu\")(head_model)\n",
    "    head_model = keras.layers.Dense(num_classes,activation='softmax')(head_model)\n",
    "    return keras.Model(inputs=base_model.input, outputs=head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, learning_rate =0.00001, optimizer = 'Adam',  fine_tune_at_layer:int=100):\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in model.layers[fine_tune_at_layer:]:\n",
    "        layer.trainable =  True\n",
    "    compile_classifier(model, learning_rate = learning_rate, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_classifier(model, learning_rate, optimizer = 'Adam'):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        model ([tensorflow.keras.Model]): classifier model\n",
    "        learning_rate ([float]): [description]\n",
    "        optimizer ([tensorflow.keras.optimizers]): optimizer\n",
    "    \"\"\"\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07,\n",
    "            amsgrad=True,\n",
    "            name=\"Adam\"\n",
    "            )\n",
    "\n",
    "    elif optimizer == 'Adagrad':\n",
    "        optimizer=tf.keras.optimizers.Adagrad(\n",
    "            learning_rate=learning_rate,\n",
    "            initial_accumulator_value=0.1,\n",
    "            epsilon=1e-07,\n",
    "            name=\"Adagrad\")\n",
    "    \n",
    "    elif optimizer == 'NestrovSGD':\n",
    "        optimizer=tf.keras.optimizers.SGD(\n",
    "            learning_rate=0.01, \n",
    "            momentum=0.9, \n",
    "            nesterov=True, \n",
    "            name=\"SGD\")\n",
    "\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "              #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              loss =tf.keras.losses.CategoricalCrossentropy(),\n",
    "              #loss=get_weighted_loss(pos_weights, neg_weights),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,train_ds,\n",
    "    validation_ds, \n",
    "    num_epochs: int = NUM_EPOCHS, batch_size: int = BATCH_SIZE):\n",
    "    history = model.fit(train_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=validation_ds,\n",
    "                    #steps_per_epoch = len(train_ds)/batch_size,#steps_per_epoch = 100, \n",
    "                    #validation_steps=len(validation_ds)/batch_size, #validation_steps= 25, \n",
    "                    callbacks=callbacks)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_and_loss(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,max(plt.ylim())])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_classifier(model_name, train_generator, valid_generator, learning_rate:float = 0.0001, fine_tune_at_layer:int = 100):\n",
    "    \n",
    "    # get base model\n",
    "    base_model = get_base_model(model_name=model_name,\n",
    "        freez_layers=True)\n",
    "\n",
    "    \n",
    "    model = add_classifier_to_base_model(base_model,\n",
    "        num_classes = NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    model = compile_classifier(model, learning_rate)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'{model_name} Training and Validation: ')\n",
    "    \n",
    "    history = fit_model(model, train_generator, \n",
    "        valid_generator, \n",
    "        num_epochs=NUM_EPOCHS)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f'{model_name} Accuracy and Loss plots')\n",
    "    plot_accuracy_and_loss(history)\n",
    "    \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    #fine_tune model_name\n",
    "    model_ft = fine_tune_model(model,learning_rate,optimizer='Adam',fine_tune_at_layer=fine_tune_at_layer)\n",
    "    #print(model_ft.summary())\n",
    "\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'Fine-Tuned {model_name} Training and Validation: ')\n",
    "    history_fine = fit_model(model_ft, train_generator, \n",
    "        valid_generator, \n",
    "        num_epochs=NUM_EPOCHS)\n",
    "    print(f'Fine-Tuned {model_name} Accuracy and Loss plots')\n",
    "    plot_accuracy_and_loss(history_fine)\n",
    "    \n",
    "    model_ft.save('my-model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # RESENET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_base_model = PRETRAINED_MODELS[0]\n",
    "learning_rate = 0.01\n",
    "fine_tune_at_layer = 0\n",
    "train_validate_classifier(pretrained_base_model,train_generator, valid_generator, learning_rate,fine_tune_at_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase II -  Reload the Saved Model and Train, Validate and test on new Covid19 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESETNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model weights\n",
    "#pretrained_base_model = PRETRAINED_MODELS[0]\n",
    "print(os.getcwd())\n",
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"my-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer = reconstructed_model.layers[-2].output\n",
    "output = keras.layers.Dense(3, activation='softmax')(new_layer)\n",
    "new_model = keras.Model(inputs = reconstructed_model.input, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  '../data/raw/covid19/Covid19-dataset/train',\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  label_mode='categorical',\n",
    "  seed=SEED,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  '../data/raw/covid19/Covid19-dataset/train/',\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  label_mode='categorical',\n",
    "  seed=SEED,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  '../data/raw/covid19/Covid19-dataset/test/',\n",
    "  label_mode='categorical',\n",
    "  seed=SEED,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of training batches {tf.data.experimental.cardinality(train_ds)}\")\n",
    "print(f\"number of validation batches {tf.data.experimental.cardinality(validation_ds)}\")\n",
    "print(f\"number of test batches {tf.data.experimental.cardinality(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_classifier(model, train_generator, valid_generator, learning_rate:float = 0.0001, fine_tune_at_layer:int = 100):\n",
    "    \n",
    "   \n",
    "    model = compile_classifier(model, learning_rate)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'Training and Validation: ')\n",
    "    \n",
    "    history = fit_model(model, train_generator, \n",
    "        valid_generator, \n",
    "        num_epochs=NUM_EPOCHS)\n",
    "\n",
    "    \n",
    "    print(f'Accuracy and Loss plots')\n",
    "    plot_accuracy_and_loss(history)\n",
    "    \n",
    "        #evaluate  model\n",
    "        \n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    print(f'Evaluation: ')\n",
    "    print(f'Test accuracy : {accuracy}')\n",
    "    print(f'Test loss : {loss}')\n",
    "\n",
    "    print(\"\\n\")\n",
    "    #fine_tune model_name\n",
    "    model_ft = fine_tune_model(model,learning_rate,optimizer='Adam',fine_tune_at_layer=fine_tune_at_layer)\n",
    "    #print(model_ft.summary())\n",
    "\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'Fine-Tuned Training and Validation: ')\n",
    "    history_fine = fit_model(model_ft, train_generator, \n",
    "        valid_generator, \n",
    "        num_epochs=NUM_EPOCHS)\n",
    "    print(f'Fine-Tuned Accuracy and Loss plots')\n",
    "    plot_accuracy_and_loss(history_fine)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    #evaluate  model\n",
    "    \n",
    "    loss_ft, accuracy_ft = model_ft.evaluate(test_ds)\n",
    "    print(f'Fine-Tuned Evaluation: ')\n",
    "    print(f'Fine-Tuned Test accuracy : {accuracy_ft}')\n",
    "    print(f'Fine-Tuned Test loss : {loss_ft}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_at_layer = 178\n",
    "train_validate_test_classifier(new_model,\n",
    "                          train_ds, \n",
    "                          validation_ds, \n",
    "                          learning_rate,\n",
    "                          fine_tune_at_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c11b3021c1881b6bbbf2c62306194f88f1d63a31bfe6dca934fc09deb5527ea"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
